---
title: "Claude Code for Fullstack Development: The 3 Things You Actually Need"
authors: [vinny]
image: /img/claude-code-fullstack/banner.png
tags: [wasp, ai, claude-code, fullstack]
---

import { ImgWithCaption } from './components/ImgWithCaption';
import SchemaMarkup from './components/SchemaMarkup';

// TODO: Add image URL to schema
<SchemaMarkup schema={{
  "@context": "https://schema.org",
  "@type": "Article",
  "headline": "Claude Code for Fullstack Development: The 3 Things You Actually Need",
  "description": "Learn the 3 essential approaches to using Claude Code effectively for fullstack development: full-stack debugging visibility, LLM-friendly documentation, and opinionated frameworks.",
  "image": "https://wasp.sh/img/claude-code-fullstack/banner.png",
  "datePublished": "2026-01-29",
  "dateModified": "2026-01-29",
  "author": {
    "@type": "Person",
    "name": "Vinny",
    "url": "https://vincanger.github.io",
    "image": "https://github.com/vincanger.png"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Wasp",
    "url": "https://wasp.sh",
    "logo": {
      "@type": "ImageObject",
      "url": "https://wasp.sh/img/wasp-logo.png"
    }
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://wasp.sh/blog/2026/01/29/claude-code-fullstack-development-essentials"
  },
  "keywords": ["Claude Code", "fullstack development", "fullstack framework", "AI coding", "vibe coding", "Wasp framework", "MCP servers", "llms.txt"]
}} />

## How Can I Use Claude Code Effectively?

There's a lot of hype around vibe coding with Claude Code.

The good news is that it's warranted. Claude Code can take care of some [surprisingly complex coding tasks](https://www.anthropic.com/news/claude-opus-4-5).

The bad news is that we're now getting bombarded with claims of amazing apps being vibe coded in a couple hours, or about complex workflows that use 10 parallel sub-agents running in a loop to replace the work of 5 software engineers.

If you're not already a Claude Code power user, all this talk can leave you feeling a bit uneasy about how to use it effectively or how to get started with it.

*Am I missing out on productivity gains by not adopting this insane new workflow? Should I be using subagents, commands, skills, or MCP servers for my use case? If so, how?*

I was asking myself these exact questions, so I did a good few weeks of research and testing, and reached the following conclusion:

**With just a few well-picked tools, and Claude Code's basic features, you have enough to "vibe code" great full-stack apps.**

## You Don't Need All its Features

While Claude Code's abilities are impressive, you quickly realize that there is a lot of feature overlap and stuff you just generally won't have to touch often if you get a few things right from the start. I'll explain what these are, and then go into more detail on each topic within this article.
{/* truncate */}

**Provide it with full-stack debugging visibility**

This allows Claude to actually see and respond to what it's coding, rather than us having to copy-paste errors, or describe issues.

**Give Claude Code access to up-to-date, LLM-friendly documentation**

This is crucial when working with LLMs that may have outdated knowledge, or hallucinate solutions that are sub-optimal or just flat out don't work.

**Use the right tech stack or framework**

This is probably the most critical of these three approaches. By picking the right framework, you give AI clear patterns to follow, and remove a lot of complexity right from the start.

With this foundation, you'll be able to build and deploy fullstack apps easily by mostly using Claude Code's default workflows, and just a couple custom commands/skills (I also packed these ideas into a simple Claude Code plugin you can install and use, but I'll talk more about that later).

This approach works because it provides guardrails and the right patterns for the agent to follow, so you can spend more time on business logic implementation, and less time working out specifications and technical details.

Essentially, you get to work with the agent on *what* you want, rather than having to explain *how* you want it, bringing that magic feeling of AI-assisted coding to complex, full-stack apps.

Let me explain.

> *Weird seeing all these complex LLM workflows and tools. Meanwhile I run one continuous Claude convo per project... I've never used a subagent. Never used MCP…
And I have wildly good results ¯\_(ツ)_/¯*  — [Chris McCord, creator of the Phoenix framework](https://x.com/chris_mccord/status/2016554559078883510)

---

## Giving Claude Code Full-Stack Vision

### Establishing a Tight Feedback Loop

Our typical AI-assisted coding workflow tends to look like this: we prompt, then wait, then look over the generated code (maybe), then check on things in the browser. If we've got some errors or some bad looking frontend designs, we copy and paste them and try to (better) explain what we want.

{/* TODO: Add video - an AI-assisted coding workflow without "full-stack vision" */}

What's worse, is we might have to do this a few times before we're satisfied, or get things working again.

By always keeping ourselves in the loop, [we're slowing things down a lot](https://www.anthropic.com/research/estimating-productivity-gains). We should, at times, just get out of the way and let the agent improve the code with each iteration until its done, before checking the result.

But to do this, we need to give Claude Code a "set of eyes". Luckily, this is possible with the right features and tools, allowing it to see the results of the code it wrote, and quickly modify it autonomously if it encounters an error anywhere in the stack.

This allows Claude to keep running in a loop until it reaches the outcome or goal you specified in your prompt. It no longer needs to *wait on you* to report back on the status of each incremental change.

Let's check out how.

## Running the Dev Server as a Background Task

Claude Code introduced their [background tasks feature](https://code.claude.com/docs/en/interactive-mode#background-bash-commands) which lets it execute long-running tasks, like app development servers (e.g. `npm run dev`), on the side without blocking Claude's progress on other work. The nice part is that Claude can continue to read and respond to the output of this task while you work.

{/* TODO: Add video demo of background tasks */}

To run commands in the background, you can either:

- Prompt Claude Code to run a command in the background, e.g. `"run my app's dev server in the background"`
- Press Ctrl+B to move a regular Bash tool invocation to the background, e.g. `"run the dev server"` + `Ctrl+b` as it starts

Even though Claude can read the output of your background tasks, there are times you may want to check up on them yourself, and you still can do that. Just use the down arrow to highlight the background task message and click enter.

This is great because now Claude can react to issues that occur from building and serving your code. Unfortunately, it still can't react to errors that occur while your app is running in the browser.

But there's a cool way to solve this.

## Browser Automation Tools

The missing piece so far is giving Claude the ability to actually *see* the result of the code it wrote, most typically what the UI looks like.

Problems that only show up **after the app is running in the browser**, like design issues and runtime errors, aren't yet visible to the agent. So this tends to be where the human steps in to report back to: *"the button is misaligned"*, *"there's a 404 error when trying to login"*, *"the console says something about `undefined`"*.

But luckily, we can arm Claude Code with browser automation tools to solve for this. These tools allow for programmatic control of the browser, like loading pages, clicking buttons, inspecting elements, reading console logs, and even taking screenshots.

{/* TODO: Add video demo of browser automation */}

This closes the loop for the *entire* stack and gives Claude the autonomy to complete larger features tasks entirely on its own.

[Chrome DevTools MCP server](https://developer.chrome.com/blog/chrome-devtools-mcp) is one of the best options currently out there, though there are many alternatives. It's easy to install, and specializes in browser debugging and performance insights.

To install it, run the following command in the terminal to add it to your current project:

```bash
claude mcp add chrome-devtools --scope project npx chrome-devtools-mcp@latest
```

Then start a new Claude session and give it a prompt like:

```
Verify in the browser that your change works as expected.
```

You should see a separate Chrome instance open up being controlled by Claude. Go ahead and give it more tasks like:

- authenticating a test user
- checking the lighthouse performance score of a site (e.g. how fast it loads)
- giving you feedback on how to improve your app's design

{/* TODO: Add video - Adding a full-stack "subtasks" feature with our "full-stack vision" workflow */}

Now when you're implementing full-stack features in your app, ask Claude to verify the feature works correctly by checking the logs in the development server, as well as in the browser with the Chrome DevTools MCP.

Alternatively, if you want Claude to *always* automatically verify changes in the browser with the DevTools MCP without having to explicitly ask it to do so, you can [add a rule to Claude's memory in your `CLAUDE.md` file](https://code.claude.com/docs/en/memory#set-up-project-memory).

---

## Give the Agent Access to the Right Docs

### The Versioning Problem

You've probably experienced this: you're vibe coding using a library's API, and the AI confidently writes code that would have worked perfectly… two versions ago. Or it creates some crazy, over-engineered work-around to a known issue that has a simple solution in its documentation.

This is because the model's training data has a cut-off date. LLMs and tools like Claude have no way of knowing about the updated code patterns, unless you give it access to the current documentation.

But, as [Andrej Karpathy observed](https://x.com/karpathy/status/1899876370492383450), most documentation contains content that's not relevant for LLMs:

> *99% of libraries still have docs that basically render to some pretty .html static pages assuming a human will click through them. In 2025 the docs should be a your_project.md text file that is intended to go into the context window of an LLM.*

There's [research to back up his claim,](https://research.trychroma.com/context-rot) showing that LLM performance degrades when irrelevant content, like HTML syntax or verbose instructions for humans, is added to context as it distracts from the task and reduces accuracy.

**In other words, every unnecessary token in your context window makes the AI slightly worse at its job.**

So we need to be giving Claude Code the *right* kind of docs.

### The Solution: Curated Doc Access

The fix is simple: give the agent the ability to fetch and read the relevant, LLM-optimized documentation for the tools you're using.

Here are the two main ways developers are accomplishing this:

- **MCP Servers** — A standard for external systems to serve data to AI agents. Popular dev tools, such as [Vercel](https://vercel.com/docs/mcp/vercel-mcp), make these available with doc searching tools.
- **llms.txt and doc maps** — A standard for publishing LLM-friendly documentation at a well-known URL, e.g. [https://wasp.sh/llms.txt](https://wasp.sh/llms.txt) — what we're using at Wasp. The agent fetches structured docs optimized for LLM context windows.

### MCP Servers for Docs Fetching

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is an open standard for connecting AI applications (i.e. agents, LLMs) to external systems. An MCP client, like Claude Code, can communicate with an MCP server, which acts as a "portal" to tools and information that the agent can use.

Think of MCP like a USB port for AI agents. Any tools or application can create an MCP server for AI agents to plug into. Each MCP server might do different things, but each MCP server is structured similarly, so the AI agent only needs one kind of "plug" to access them all.

{/* TODO: Add image - MCP diagram */}

There are already tons of MCP servers for popular tools, like Supabase, Jira, Canva, Notion, and Vercel. Claude Code has [a docs section listing these and many more](https://code.claude.com/docs/en/mcp#popular-mcp-servers), with instructions on how to install them if you're interested.

{/* TODO: Add image - MCP servers list screenshot */}

Developer Tool MCP servers like Supabase and Vercel expose tools like `search_docs` which will take a query from the agent, fetch relevant files and guides on the MCP server, and return the relevant information back to the agent. Because the doc fetching is done on the MCP server, it can do a number of things before sending that information back, like filtering, attaching metadata, check for the proper doc version, and more.

There are some pros and cons to this approach, though.

**Pros**

- can do some pre-processing on the docs before sending them back
- can filter and fetch relevant snippets across multiple docs/guides
- return structured outputs instead of raw text
- can be paired with other tool calls in the same MCP server, e.g. Supabase's `list_projects`

**Cons**

- the MCP server decides what is relevant, ignoring potentially useful information
- all of its tools (not just doc fetching tools) get loaded into, and quickly fill up, the LLM's context window (i.e. "memory"), degrading the agent's performance
- more overhead for the agent: evaluate prompt → find the right MCP tool → call it → wait for response from MCP server → evaluate response → take action

Because LLMs don't have a real memory, they have to load information into context every session. All the MCP server tools available to an agent like Claude Code always get loaded into its context. The agent searches through this list to see if there's a suitable tool it can call when processing each request.

{/* TODO: Add image - Context window usage tip screenshot */}

:::tip
Run the `/context` slash command in an active Claude Code session to see how much of your context window is already used up, and proportionally by each feature.
:::

A single MCP server can add around 15-30 tools to the context, and with multiple servers, you can easily consume 10-20% or more of your LLM's context window (i.e. "available memory") before sending any messages or prompts due to the limited size of an LLM's context window

Once a context window gets close to full, it's performance degrades significantly. Some people even suggest starting a new session once the context is 75% full to avoid this, which is possible with the `/clear` command in Claude Code. You can also run the `/compact` command which creates a summary of your current session's context and passes it along to the next session.

There are also other attempts to solve this issue, such as Claude's new [Tool Search tool](https://platform.claude.com/docs/en/agents-and-tools/tool-use/tool-search-tool), which will search and load tools on-demand, rather than loading all MCP tools upfront, but this adds even more overhead to the agent.

Luckily, if your main reason for using an MCP server is just for documentation searching, then an alternative open standard exists that may be a better fit.

### LLM-friendly Docs URLs — LLMs.txt

[LLMs.txt](https://llmstxt.org/) has quickly become the standard for providing LLMs with context-friendly versions of websites. The website maintainers add a concise, text-only markdown file at a `/llms.txt` path which the LLM can easily navigate.

Developer tools and the like often use it to provide a map of links to their raw documentation text files, among other links and references. Try it out on some of your favorite developer tools URLs, such as:

- [stripe.com/llms.txt](https://stripe.com/llms.txt)
- [supabase.com/llms.txt](https://supabase.com/llms.txt)
- [wasp.sh/llms.txt](https://wasp.sh/llms.txt)

{/* TODO: Add video - Vercel versus Supabase's llms.txt URLs */}

While some llms.txt URLs serve as a simple map to further documentation, others provide a complete list of references. The differences can be pretty extreme!

But although llms.txt files might vary widely in the content they surface, they always follow the same format of a simple markdown file with the title of the website and some links. That's it!

```markdown
# Title

> Optional description goes here

Optional details go here

## Section name

- [Link title](https://link_url): Optional link details

## Optional

- [Link title](https://link_url)
```

This allows LLMs to get precise information without all the fluff. In contrast, if they fetch a normal website URL, an LLM has to convert complex HTML with navigation, ads, and javascript into something it can make use of.

Here are some of the pros and cons of using llms.txt as a documentation fetching method for coding agents:

**Pros**

- curated list of the most relevant information for LLMs / AI agents
- extremely simple; works with any agent that can fetch URLs
- agents decide which links to follow and can see full source text
- very context window efficient

**Cons**

- raw docs / markdown files may transmit more information than needed
- the agent could fetch incompatible links / files
- the agent might need more guidance / rules on how to interact with the docs correctly

In my opinion, I think that fetching docs via an `llms.txt` URL is the better approach. It doesn't use up any of the context window until its fetched, and once it is the token size of a typical documentation file is ~100 tokens versus 5,000-10,000 tokens for just one MCP server.

**That's a 10x reduction in context usage.**

Plus, you get the added benefit that `llms.txt` files are easier for humans to reference as well.

{/* TODO: Add image - llms.txt example */}

Even though the agent is responsible for reading and reasoning about the docs correctly, they've gotten *very* good at this recently, so it's not the drawback that it might sound like. It's more than enough to give them the the documentation map `llms.txt` URL, and let them navigate it and fetch the links relevant to the current task.

This means that you may need to explicitly provide Claude with the correct URL for the tools you're using, and perhaps some short rules about how you'd prefer it to retrieve doc files in your `CLAUDE.md` (but you can think of this as being analogous to installing an MCP server).

It's actually the method that Claude Code uses for its [own docs](https://simonwillison.net/2025/Oct/24/claude-code-docs-map/). So when you ask it a question about its own features, like "How can I create a new Skill?" or "How do I start a background task in Claude Code?", it will first fetch its own [documentation map markdown file URL](https://simonwillison.net/2025/Oct/24/claude-code-docs-map/), find the correct guides, and then retrieve them from their respective URLs before giving you an answer.

The question now is, which tools might you actually be needing to provide the docs for? And the most obvious answer is the tech stack or framework you're building your full-stack with.

---

## Choosing the Right Tech Stack / Framework

### Popular Choices

This is probably the most important of the 3 pillars. Starting with a tech stack that AI can easily reason about will make the job of building the app you want significantly easier.

There are a lot of options out there, but luckily there are many solid choices, such as:

- [NextJS](https://nextjs.org/docs) (React, NodeJS)
- [Laravel](https://laravel.com/docs) (PHP)
- [Ruby on Rails](https://rubyonrails.org/docs) (Rails)
- [Wasp](https://wasp.sh) (React, NodeJS, Prisma) — the framework we're building

In 2026, you'll be able to get very far with Claude Code and any of the frameworks listed above. But while these frameworks all offer good conventions, and are responsible for stitching together the most important parts of the stack, most of these still require some sort of additional integration.

### Which one should you go with?

For NextJS, which is more client-side focused, you have to chose and connect your own database layer. For Laravel and Rails, which unify backend and database, you need to decide which client to use and how it will talk to your backend.

That's why a lot of developers will reach for popular stacks/combos that include these frameworks, such as:

- NextJS + tRPC + Prisma + NextAuth (aka [T3 Stack](https://create.t3.gg/en/introduction))
- Laravel + Inertia.js + React ([Laravel Inertia Stack](https://inertiajs.com/docs/introduction))
- Rails + Hotwire ([Rails Hotwire Stack](https://hotwired.dev/))

You might have noticed as well that the T3 Stack is the only one to include an authentication library, NextAuth. That's because the backend-focused frameworks, Laravel and Rails, have an opinionated way of adding authentication to your app already, but NextJS does not.

Wasp, on the other hand, is the only one of the bunch that unifies all parts of the stack — client ↔ backend ↔ database — while also being opinionated on features.

**This is all great, but which one should you ultimately choose?**

### The More Opinionated, The Better

Well, the more opinionated the framework, the better it vibes with AI.

If a framework is **opinionated**, it means there's usually one obvious place to put code and one common pattern to follow. The AI doesn't have to guess. They've already made decisions ahead of time so you (and your agent) don't have to. They've encoded architectural wisdom into conventions, picked the libraries to use, the way authentication is wired, and how the app should be structured.

**So when there are fewer decisions to make, less boilerplate to write, and fewer tools to stitch together, the development process becomes more reliable.**

And as the app gets more complex, AI doesn't lose its focus, because the framework is handling a lot of underlying complexity.  You can also understand and inspect what is being generated more easily, and avoid building yourself into a messy corner.

Consider that **opinionated frameworks can reduce boilerplate by 60-80%**. Wasp's auth declaration, for example, replaces 500+ lines of typical authentication code with a 10 to 15-line config. That's 97% less code that needs to be generated and audited!

Of these frameworks, Wasp is definitely the most opinionated but it's also the newest kid on the block. After that, Laravel and Rails are probably tied for a close second, but they are built on PHP and Rails, respectively, and come with their own distinct ecosystems, so you'll most likely have to pair them with a frontend JavaScript framework like React, too. NextJS, is the most popular but least opinionated of the bunch, so it means there is more complexity and up-front choices you and your AI have to deal with, but this offers more flexibility in the long run.

So in the end, the choice you make largely depends on what you're trying to achieve, what you're comfortable with, and how much flexibility you need.

Just remember, the more a framework gives you structure and defaults, the easier it is for AI to generate code that fits correctly — and the less time you spend fixing confusing or inconsistent output.

## What This Means for Claude Code

Ok. So we've established that working with an opinionated framework means that it it manages a lot of the complexity for you.

But what does that practically mean when using it with Claude Code?

- **Subagents for architecture planning?** The framework already decided the architecture.
- **Elaborate plans for where code should live?** The conventions tell you.
- **Back-and-forth to agree on patterns?** The patterns are already decided.
- **Glueing the pieces of your app together?** The frameworks manage this code.
- **Context that explains your app structure?** It's embedded in the framework.

In a sense, the framework acts as a large specification that both you and Claude already understand and agree on.

**Instead of multi-turn conversations to figure out HOW things should be built, you get to just say WHAT you want built.**

{/* TODO: Add image - Wasp visual representation of full-stack app */}

:::info
[Wasp even gives you a visual representation](https://wasp.sh/blog/2024/04/25/first-framework-that-lets-you-visualize-react-node-app-code) of your full-stack app that an opinionated framework provides to AI
:::

So when you tell Claude "add a new model for Comments" or "add a user account settings feature", Claude will know exactly what that means and where it all goes. And you also get the added benefit that the implementations follow best practices and are backed by the decisions of experienced professionals behind the framework, and is not just some [LLM hastily implementing a feature on the wrong assumptions.](https://x.com/karpathy/status/2015883857489522876)

Research shows that LLMs generate 'confident but incorrect responses' when context contains distractors, with hallucination rates increasing as context grows. An opinionated framework acts as a buffer against these confident mistakes.

This isn't to say that you no longer have to do good planning, create a good spec, or Product Requirement Doc for your agent to follow. This can still be a really important step when vibe coding (or practicing "[spec-driven development](https://martinfowler.com/articles/exploring-gen-ai/sdd-3-tools.html)"). But it does mean that, with an opinionated full-stack framework, much less of your planning phases need to be devoted to discussing architectural and technical implementation details.

---

## This Approach, In a Plugin

If you want to put this theoretical approach I discussed above to the practical test, then I suggest you try out the [Wasp plugin we created for Claude Code](https://github.com/wasp-lang/claude-plugins/tree/main/plugins/wasp).

We, the Wasp framework creators, maintain the plugin, so we've battle tested it with Wasp. Plus we're a very responsive community and we're listening to feedback and improving it all the time.

Here's how to get started:

1. **Install Wasp**

```bash
curl -sSL https://get.wasp.sh/installer.sh | sh
```

2. **Install the Claude Code plugin**

```bash
# add the Wasp marketplace to Claude
claude plugin marketplace add wasp-lang/claude-plugins

# install the plugin from the marketplace
claude plugin install wasp@wasp-plugins --scope project
```

3. **Start a new Wasp project**

```bash
# create a new project
wasp new

# change into the project root directory
cd <your-wasp-project>
```

4. **Start a Claude Code session** in your Wasp project directory

```bash
claude
```

5. **Run the init command** to set up the plugin

```bash
/wasp:init
```

From there, you can tell Claude to "start the dev server" and it will walk you through spinning up fullstack visibility as we outlined above. Or ask it to implement a Wasp feature and watch it fetch version-matched documentation guides for you!

{/* TODO: Add image - Plugin in action screenshot */}

More importantly, Wasp as a framework pushes into territory that's even more AI-native than the rest, because of its central configuration file(s) where the app is defined.

This main config file is like your app's blueprint. Wasp takes these declarations, and manages the code for those features for you.

Take this example config file authentication snippet as reference:

```wasp
app TaskManager {
  wasp: { version: "^0.21.0" },
  title: "Task Manager",
  auth: {
    userEntity: User,
    methods: {
      email: {},
      google: {}
    },
    onAuthFailedRedirectTo: "/login"
  }
}
```

This is what an authentication implementation in Wasp looks like. That's it.

This 8-line config generates what would typically require 500-800 lines of code: components, session handling, password hashing, OAuth flows, and database schemas. Claude just needs to know what auth methods you want.

Claude doesn't need to worry about choosing what kind of auth implementation to use, or generating any of the glue code. It can just get straight to work building features.

---

## When You DO Need the Fancy Stuff

I've spent this whole article arguing that you can ignore a lot Claude Code features for fullstack app development. But I don't want to leave you with the impression that those features are useless.

Custom subagents, commands, and skills shine when you're doing the same task repeatedly with consistent criteria, e.g.:

- **Testing** — A dedicated test-runner subagent that knows your testing patterns, runs suites, analyzes failures, and suggests fixes.
- **Code / PR Reviews** — A code review command with your team's specific standards baked in, checking for security issues, performance concerns, and style consistency across every pull request.
- **Running Scripts** — Skills can be useful if you have deterministic tasks you run often. Define them in a Skill and link those scripts to them, and Claude Code will run them when it deems it fit for the task at hand.

These are tasks where you want the same process followed every time and where a well-configured subagent with specific rules makes sense.

In most cases, reach for complexity only when the simpler approach stops working.

---

## Now Build!

With these three ingredients I think most fullstack app developers can get the vast majority of the work done without reaching for much more:

1. An opinionated framework that handles architecture/boilerplate so Claude doesn't have to
2. Version-matched documentation so Claude has up-to-date implementation details
3. Full-stack visibility so Claude can see what's happening and fix it on its own

With these in place, Claude Code's basic toolset—exploring, planning, reading, writing, running commands—is enough to build real, complex fullstack applications. The subagents, hooks, plugins, and complex configurations are there if you need them, but honestly most of the time, you won't.
